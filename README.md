# News-Article-Generation-using-LLM-s
Llama 2 LLM for news article generation based on data obtained by media stack API.  

# Instructions to run the notebook
You can run the notebook in **Google Colab**, make sure to chane the runtime to GPU for training and testing or Use the link to access the notebook and run all: https://colab.research.google.com/drive/1Pv9w_EMjEAwk9HsJigtzMimgOSZ95eXS?usp=sharing

# Implementation steps
1. **Installing Required Packages:**
    - accelerate, peft, bitsandbytes, and transformers are installed using the pip command.

2. **API Request to Fetch Articles:**
   - The script fetches news articles using the **MediaStack API** with specified parameters.

3. **Data Preprocessing:**
   - The fetched articles are preprocessed using a custom function (preprocess_text). The processed text is then organized into a dataset.

4. **Model and Fine-Tuning Parameters:**
   - The script defines various parameters for fine-tuning, such as model names, **QLoRA parameters, bitsandbytes parameters, training arguments, SFT parameters, etc**.

5. **Loading and Configuring Model:**
   - The script loads the base language model using the specified model name and configures it with BitsAndBytes quantization.

6. **Loading Tokenizer and LoRA Configuration:**
   - **Tokenizer** and **QLoRA configuration** are loaded based on the model name.

7. **Setting Training Parameters:**
   - Training arguments are set, including output directory, number of epochs, batch sizes, learning rate, and other training-related parameters.

8. **Supervised Fine-Tuning (SFT):**
   - The SFTTrainer is instantiated with the model, dataset, and training arguments. The model is then fine-tuned using the train method.

9. **Inference:**
   - A sample prompt **("city!")** is given to the fine-tuned model, and the generated text is printed.

# Dataset Formatting to train Llama 2 model
As we need to follow the template of Llama 2 model for fine tuning, Lets create the data based on **Title** and **Description** of a news article.
1. **Importance of Prompt Templates:**
   - Prompt templates structure inputs: system prompt, user prompt, additional inputs, and model answer.
   - Different templates (e.g., Alpaca, Vicuna) have varying impacts.
   - Converting instruction dataset to Llama 2's template is important.
   - Llama 2's template example:
>```
> {"text": "input_text", "summary": "output_text"}
> {"text": "another_input_text", "summary": "another_output_text"}
> ...

In our case, a sample from our dataset:
>```
>### title: thousands of wild horses creating hazard for nevada drivers as collisions nearly double. ### 
>description: thousands of wild horses in the virginia range near lake tahoe are becoming increasingly hazardous to drivers â€” but residents have no legal means to control the population on their own.

# Results
After fine tuning the model, I have tested the model on the following prompt:
>```
>prompt = 'city!'

The text generated by the model:
>```
>city!
>everybody's talking about it!
>the city of santa cruz is a magical place located on the central coast of california. with stunning natural beauty, rich history, and vibrant culture, santa cruz is the perfect destination for travelers looking for an authentic california experience. from the >redwood forests to the sandy beaches, from the artistic community to the farmstands, santa cruz has it all. whether you're a local looking for the latest news and events or a traveler planning your trip, make sure to check out our comprehensive coverage of >everything that makes santa cruz special.

The output of the model talks about the city of santa cruz and the quality of the text generated is really extraodinary.

# Intuition and possibilities of Improvement
Llama 2 is pretrained on a massive amount of data, enabling it to capture complex patterns and relationships in language. This pretrained knowledge can be fine-tuned on a specific dataset for better performance in domain-specific tasks. It has a higher capacity to learn abstract concepts and generate coherent and contextually relevant text. This is beneficial for tasks that require nuanced language understanding and creative text generation.

With additional resources, I can enhance the Llama 2 fine-tuning process by focusing on three key aspects: model size, training data, and training duration. Firstly, consider utilizing a larger variant of Llama 2, if available, to benefit from increased model capacity and potentially capture more intricate patterns in the data. Secondly, augment the training dataset with diverse and representative examples, ensuring a broader coverage of the target domain. Collecting a more extensive and varied dataset can help the model generalize better. Lastly, extend the training duration by increasing the number of epochs or utilizing distributed training across multiple GPUs to allow the model to converge to a more refined state. Keep an eye on performance metrics during training to identify the optimal configuration. Additionally, exploring advanced techniques like mixed-precision training or experimenting with different hyperparameter settings can contribute to further improvements.
